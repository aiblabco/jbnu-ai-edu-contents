{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN LSTM MODEL - AQI Forecasting**\n",
    "\n",
    "*The RNN LSTM model for Air Quality Index (AQI) forecasting is a machine learning approach that uses historical air quality data to predict future air quality levels. Recurrent neural networks (RNN) and Long Short-Term Memory (LSTM) models are used to analyze the complex temporal relationships within the data and make accurate predictions. This method has practical applications in pollution control and public health, allowing decision-makers to take proactive steps to reduce air pollution and its harmful effects on the environment and human health.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "# Ignore harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import HTML,display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the daily city-wise data\n",
    "df= pd.read_csv(\"/mnt/share/datasets/fala/air-quality-dataset/city_day.csv\",parse_dates=True)\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeping only the columns needed\n",
    "df=df[['City','Date','AQI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi = df[df['City'] == 'Delhi']\n",
    "delhi.drop(['City'],axis=1,inplace = True)\n",
    "delhi.set_index('Date', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill NULL DATA\n",
    "delhi['AQI'].fillna(method = 'bfill',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking monthly average\n",
    "delhi=delhi.astype('float64')\n",
    "delhi=delhi.resample(rule='MS').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=delhi[['AQI']].plot(figsize=(12,8),grid=True,lw=2,color='Red')\n",
    "ax.autoscale(enable=True, axis='both', tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORECASTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi_AQI=delhi['AQI']\n",
    "result=seasonal_decompose(delhi_AQI,model='multiplicative')\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN-LSTM MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the training and testing data\n",
    "\n",
    "train=delhi[:48]\n",
    "test=delhi[48:61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Shape of train data:\" + str(train.array.shape))\n",
    "#print(\"Shape of test data:\" + str(test.array.shape))\n",
    "\n",
    "print(\"Shape of train data:\" + str(train.shape))\n",
    "print(\"Shape of test data:\" + str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_arr=np.expand_dims(train, axis=1)\n",
    "#test_arr=np.expand_dims(test, axis=1)\n",
    "\n",
    "train_arr = train\n",
    "test_arr = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the dataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train = scaler.transform(train_arr)\n",
    "scaled_test = scaler.transform(test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining our time series generator that would be used to parse data into the model\n",
    "\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "n_input = 12\n",
    "n_features = 1\n",
    "generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = generator[0]\n",
    "print(f'Given the Array: \\n{X.flatten()}')\n",
    "print(f'Predict this y: \\n {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary components to define our model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the stacked LSTM model\n",
    "model = Sequential(name = 'LSTM-Model')\n",
    "model.add(LSTM(32, activation='relu', return_sequences = True, input_shape=(n_input, n_features),name = \"LSTM_1\"))\n",
    "#model.add(LSTM(100, activation='relu', return_sequences = True,name = \"LSTM_2\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(32, activation='relu',name = \"LSTM_2\"))\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Dense(8))\n",
    "model.add(Dense(1,name = \"DENSE_1\"))\n",
    "model.compile(optimizer= 'adam',  loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the layer structure\n",
    "from keras.utils import plot_model\n",
    "plot_model(model,show_shapes=True,show_layer_names=True,rankdir=\"TB\",dpi=96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting our model\n",
    "model.fit_generator(generator,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss vs #of epoch \n",
    "loss_per_epoch = model.history.history['loss']\n",
    "plt.plot(range(len(loss_per_epoch)),loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting using our model on the testing dataset\n",
    "test_predictions = []\n",
    "\n",
    "first_batch = scaled_train[-n_input:]\n",
    "current_batch = first_batch.reshape((1, n_input, n_features))\n",
    "\n",
    "for i in range(len(test)):\n",
    "      \n",
    "    current_pred = model.predict(current_batch)[0]\n",
    "    test_predictions.append(current_pred) \n",
    "    \n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = scaler.inverse_transform(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_predictions.shape)\n",
    "print(test_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr['Predictions'] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the true values vs predicted values on testing dataset\n",
    "plt.plot(test_arr['AQI'],label = 'AQI')\n",
    "plt.plot(test_arr['Predictions'],label = 'Predictions')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the root mean squared error on the testing dataset\n",
    "RMSE_test=np.sqrt(mean_squared_error(test_arr['AQI'],test_arr['Predictions']))\n",
    "print('RMSE on the testing dataset = ',RMSE_test)\n",
    "print('India_AQI=',test_arr['AQI'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting using our model on the training dataset\n",
    "train_predictions = []\n",
    "\n",
    "first_eval_batch = scaled_train[:n_input]\n",
    "current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "\n",
    "for i in range(len(train-n_input)):\n",
    "      \n",
    "    current_pred = model.predict(current_batch)[0]\n",
    "    train_predictions.append(current_pred) \n",
    "    \n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = scaler.inverse_transform(train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_predictions.shape)\n",
    "print(train_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr['Predictions'] = train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the true values vs predicted values on training dataset\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train_arr['AQI'],label = 'AQI')\n",
    "plt.plot(train_arr['Predictions'],label = 'Predictions')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FURTHER IMPLEMENTATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the root mean squared error on the testing dataset\n",
    "RMSE_train =np.sqrt(mean_squared_error(train_arr['AQI'],train_arr['Predictions']))\n",
    "print('RMSE on the training dataset = ',RMSE_train)\n",
    "print('India_AQI=',train_arr['AQI'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi_arr= delhi\n",
    "scaled_delhi = scaler.transform(delhi_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the future predictions using our model\n",
    "fut_predictions = []\n",
    "\n",
    "first_eval_batch = scaled_delhi[-n_input:]\n",
    "current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "\n",
    "for i in range(12):\n",
    "    \n",
    "    current_pred = model.predict(current_batch)[0]\n",
    "    \n",
    "    \n",
    "    fut_predictions.append(current_pred) \n",
    "    \n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fut_predictions = scaler.inverse_transform(fut_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fut_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_pred = np.concatenate((delhi_arr,fut_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(complete_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f263b9edec441361bf4e854813fdc9acb37a94c837e0bb7b1f3c4b82a94101de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
