{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68f91cb",
   "metadata": {},
   "source": [
    "# Multiclass Image Classification Using Alexnet Architecture\n",
    "\n",
    "This code does image classification for 7 fruit categories = ['apple', 'cabbage', 'carrot', 'cucumber', 'eggplant', 'pear', 'zucchini']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7879d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Activation,GlobalAveragePooling2D, Dense, BatchNormalization, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d1fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Training dir paths\n",
    "train_path = '/mnt/share/datasets/fruits-image-classfication/Dataset/Train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Test dir paths        \n",
    "test_path = '/mnt/share/datasets/fruits-image-classfication/Dataset/Dataset/Test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e6537",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0673db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the image\n",
    "img = load_img(train_path + \"Apple/r0_0.jpg\", target_size=(227,227))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the shape of the image array \n",
    "x = img_to_array(img)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d747517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing more Images from each class\n",
    "images = ['apple', 'cabbage', 'carrot', 'cucumber', 'eggplant', 'pear', 'zucchini']\n",
    "fig = plt.figure(figsize =(10,5))\n",
    "for i in range(len(images)):\n",
    "    ax = fig.add_subplot(3,3,i+1,xticks=[],yticks=[])\n",
    "    plt.title(images[i])\n",
    "    plt.axis(\"off\")\n",
    "    ax.imshow(load_img(train_path + images[i] +\"/r0_0.jpg\", target_size=(227,227)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the count of images for each class\n",
    "image_count = []\n",
    "class_names = []\n",
    "print('{:18s}'.format('class'), end='')\n",
    "print('Count:')\n",
    "print('-' * 24)\n",
    "#Reading the image from each folder from training path\n",
    "for folder in os.listdir(train_path):\n",
    "    folder_num = len(os.listdir(os.path.join(train_path,folder)))\n",
    "    image_count.append(folder_num)\n",
    "    class_names.append(folder)\n",
    "    print('{:20s}'.format(folder), end=' ')\n",
    "    print(folder_num)\n",
    "print('-' * 24)    \n",
    "print(\"Number of classes : \",len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eaba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the count of images for each class\n",
    "sns.set(rc={'figure.figsize':(5,5)})\n",
    "sns.barplot(x=class_names, y=image_count)\n",
    "plt.ylabel('Number of images in each class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a69c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding number of classes\n",
    "className = glob(train_path + '/*')\n",
    "num_classes = len(className)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63197cb8",
   "metadata": {},
   "source": [
    "# Model using Alexnet architecture\n",
    "\n",
    "This model consist of 5 convolution layers followed by 3 fully connected layers. Ativation function used is relu. The network consists of a kernel or filters with size 11 x 11, 5 x 5, 3 x 3, 3 x 3 and 3 x 3 for its five convolutional layers respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb29836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Alexnet architecture. input is 227, 227,3 image and output is 7 classes of fruits \n",
    "model = tf.keras.models.Sequential([\n",
    "    #1st Convolutional Layer\n",
    "    tf.keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    #2nd Convolutional Layer\n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    #3rd Convolutional Layer\n",
    "    tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    #4th Convolutional Layer\n",
    "    tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    #5th Convolutional Layer\n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    #Passing it to a Fully Connected layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 1st Fully Connected Layer\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),# Add Dropout to prevent overfitting\n",
    "    # 2nd Fully Connected Layer\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    # 3rd Fully Connected Layer\n",
    "    tf.keras.layers.Dense(1000, activation='relu'),\n",
    "    #tf.keras.layers.BatchNormalization(),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    #Output Layer\n",
    "    tf.keras.layers.Dense(7, activation='softmax'),\n",
    "    #tf.keras.layers.BatchNormalization()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph\n",
    "plot_model(model, to_file='fruit_classification.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297c2550",
   "metadata": {},
   "source": [
    "# Compiling the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.SGD(learning_rate=0.00001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061996f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting model's summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981aa0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifing epochs & batch size\n",
    "epochs = 40\n",
    "batch_size = 16\n",
    "image_height = 227\n",
    "image_width = 227"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ba84b",
   "metadata": {},
   "source": [
    "# Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733bde2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an object of ImageDataGenerator for augmenting training dataset\n",
    "train_datagen = ImageDataGenerator(rescale= 1./255,\n",
    "rotation_range=10,\n",
    "width_shift_range=0.1,\n",
    "height_shift_range=0.1,\n",
    "shear_range=0.1,\n",
    "zoom_range=0.1,\n",
    "horizontal_flip=True,\n",
    "fill_mode='nearest')\n",
    "\n",
    "#Creating an object of ImageDataGenerator for augmenting test dataset\n",
    "test_datagen = ImageDataGenerator(rescale= 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c766deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating batches of Augmented data of image size 227, 227 and batch size of 32\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "directory = train_path, \n",
    "target_size= (image_height, image_width), # resize to this size\n",
    "batch_size = batch_size,\n",
    "color_mode= \"rgb\",\n",
    "class_mode= \"categorical\"\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "directory = test_path,\n",
    "target_size=(image_height, image_width),\n",
    "batch_size = batch_size,\n",
    "color_mode= \"rgb\",\n",
    "class_mode= \"categorical\")\n",
    "\n",
    "nb_train_samples = train_generator.samples\n",
    "nb_test_samples = test_generator.samples\n",
    "classes = list(train_generator.class_indices.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753bbd18",
   "metadata": {},
   "source": [
    "# Creating callback list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49854677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "#Callback to save the best model. Using checkpoint and earlystopping to monitor validation accuracy\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', factor=0.1, patience=10, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='fruit_model.h5',\n",
    "        monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10,verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d0983a",
   "metadata": {},
   "source": [
    "# Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc55109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "#Training\n",
    "history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks = callbacks_list,\n",
    "        validation_data=test_generator,\n",
    "        verbose = 1,\n",
    "        validation_steps=nb_test_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8024c6",
   "metadata": {},
   "source": [
    "# Accuracy and loss Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec777576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the loss function and accuracy for different epochs\n",
    "\n",
    "plt.figure(1, figsize = (10, 10))  \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['accuracy'])  \n",
    "plt.plot(history.history['val_accuracy'])  \n",
    "plt.title('Model Accuracy')  \n",
    "plt.ylabel('Accuracy')  \n",
    "plt.xlabel('Epoch')  \n",
    "plt.legend(['train', 'validation'], loc='upper left')   \n",
    "\n",
    "# plotting model loss \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('Model Loss')  \n",
    "plt.ylabel('Loss')  \n",
    "plt.xlabel('Epoch')  \n",
    "plt.legend(['train', 'validation'], loc='upper left')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e320de4a",
   "metadata": {},
   "source": [
    "# Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c41533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "score = model.evaluate(test_generator)\n",
    "print('test loss:', score[0])\n",
    "print('test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382ebac",
   "metadata": {},
   "source": [
    "# Model Saving and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the whole model\n",
    "model.save(\"./fruit_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction for a new image.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path='./Dataset/Dataset/new_samples/'\n",
    "img = load_img(sample_path + \"r0_6.jpg\", target_size=(227,227))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(filename):\n",
    "    #load the image\n",
    "    img = load_img(filename, grayscale=False, color_mode=\"rgb\", target_size=(227, 227, 3))\n",
    "    #convert to array\n",
    "    img = img_to_array(img)\n",
    "    #reshape into a single sample with 1 channel\n",
    "    img = img.reshape(1, 227, 227, 3)\n",
    "    #prepare pixel data\n",
    "    img = img.astype('float32')\n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1255bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_image(sample_path + \"r0_2.jpg\")\n",
    "model = load_model('fruit_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the class\n",
    "predict_x=model.predict(img)\n",
    "result=np.argmax(predict_x,axis=1)\n",
    "if result[0] == 0:\n",
    "    print(\"Apple\")\n",
    "elif result[0] == 1:\n",
    "    print(\"cabbage\")\n",
    "elif result[0] == 2:\n",
    "    print(\"carrot\")\n",
    "elif result[0] == 3:\n",
    "    print(\"cucumber\")\n",
    "elif result[0] == 4:\n",
    "    print(\"eggplant\")\n",
    "elif result[0] == 5:\n",
    "    print(\"pear\")\n",
    "elif result[0] == 6:\n",
    "    print(\"zucchini\")\n",
    "else:\n",
    "    print(\"Not in the list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690155a",
   "metadata": {},
   "source": [
    "This classification model gives test accuracy of 80.73% and when tried on few sample images, it classifies them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30026072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
