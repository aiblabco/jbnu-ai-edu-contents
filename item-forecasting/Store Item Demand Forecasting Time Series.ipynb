{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Basic packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Time Series\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"/mnt/share/datasets/item-forecasting/train.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting Date from string to datetime\n",
    "train_df['date'] = pd.to_datetime(train_df['date'], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()           # no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand dataframe with more useful columns\n",
    "def expand_df(df):\n",
    "    data = df.copy()\n",
    "    \n",
    "    data['day'] = data.date.dt.day\n",
    "    data['month'] = data.date.dt.month\n",
    "    data['year'] = data.date.dt.year\n",
    "    data['dayofweek'] = data.date.dt.dayofweek\n",
    "    return data\n",
    "\n",
    "train_df = expand_df(train_df)\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.set_index('date')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(x=\"date\",y=\"sales\",legend=\"full\",data=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposing Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The Seasonal component:** A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known frequency. A time series can contain multiple superimposed seasonal periods.\n",
    "\n",
    "- **The Trend component:** A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes a trend is referred to as “changing direction” when it might go from an increasing trend to a decreasing trend.\n",
    "\n",
    "- **The Cyclical component:** The cyclical component represents phenomena that happen across seasonal periods. Cyclical patterns do not have a fixed period like seasonal patterns do. The cyclical component is hard to isolate and it's often ‘left alone’ by combining it with the trend component.\n",
    "\n",
    "- **The Noise component:** The noise or the random component is what remains behind when you separate out seasonality and trend from the time series. Noise is the effect of factors that you do not know, or which you cannot measure. It is the effect of the known unknowns, or the unknown unknowns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify additive or multiplicative model for decomposition-       \n",
    "There are basically two methods to analyze the seasonality of a Time Series: additive and multiplicative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use multiplicative models when the magnitude of the seasonal pattern in the data depends on the magnitude of the data. In this situation, trend and seasonal components are multiplied and then added to the error component. It is not linear, can be exponential or quadratic.\n",
    "\n",
    "On other hand, in the additive model, the magnitude of seasonality does not change in relation to time. In this the effects of the individual factors are differentiated and added to model the data. In this situation, the linear seasonality has the same amplitude and frequency. \n",
    "\n",
    "Depending on whether the composition is multiplicative or additive, we’ll need to divide or subtract the trend component from the original time series to retrieve the seasonal and noise components.\n",
    "\n",
    "The **additive model** is Y[t] = T[t] + S[t] + e[t]\n",
    "\n",
    "The **multiplicative model** is Y[t] = T[t] * S[t] * e[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets decompose for data of smaller size. Here I will take data having item and store equal to 1.\n",
    "\n",
    "train_item1 = train_df[train_df['item']==1]\n",
    "train_final = train_item1[train_item1['store']==1]\n",
    "\n",
    "#from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = sm.tsa.seasonal_decompose(train_final['sales'], model='additive', period=365) #Seasonal decomposition using moving averages\n",
    "\n",
    "fig = plt.figure()  \n",
    "fig = result.plot()  \n",
    "fig.set_size_inches(14, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying any statistical model on a Time Series, the series has to be stationary or time invariant, which means that, over different time periods, it should have constant means, constant variance and constant covariance. It means that the data should have constant mean throughout, scattered consistently and should have same frequency throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The mean of the series should not be a function of time.\n",
    "- The variance of the series should not be a function of time. This property is known as homoscedasticity. \n",
    "- Finally, the covariance of the i th term and the (i + m) th term should not be a function of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to check the stationarity using 2 methods:\n",
    "\n",
    "1. **Rolling Mean:** Plot the moving average or moving standard deviation to see if it varies with time.\n",
    "\n",
    "2. **ADCF Test — Augmented Dickey–Fuller test:** This is used to gives us various values that can help in identifying stationarity. The Null hypothesis says that a Time-series is non-stationary. It comprises of a Test Statistics & some critical values for some confidence levels. If the Test statistics is less than the critical values, we can reject the null hypothesis & say that the series is stationary. The ADCF test also gives us a p-value. According to the null hypothesis, lower values of p is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Mean Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_stats(timeseries, window = 12, cutoff = 0.01):\n",
    "    \n",
    "    rolmean = timeseries.rolling(window).mean()\n",
    "    rolstd = timeseries.rolling(window).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    fig = plt.figure(figsize=(16, 4))\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "   # plt.rcParams['agg.path.chunksize'] = 50000\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    \n",
    "roll_stats(train_final['sales'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dickey-Fuller test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dickey_fuller_test(timeseries, window = 12, cutoff = 0.01):\n",
    "    dftest = adfuller(timeseries, autolag='AIC', maxlag = 20 )\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    pvalue = dftest[1]\n",
    "    if pvalue < cutoff:\n",
    "        print('p-value = %.4f. The series is likely stationary.' % pvalue)\n",
    "    else:\n",
    "        print('p-value = %.4f. The series is likely non-stationary.' % pvalue)\n",
    "    \n",
    "    print(dfoutput)\n",
    "    \n",
    "dickey_fuller_test(train_final['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the time series stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff = train_final.sales - train_final.sales.shift(1)\n",
    "first_diff = first_diff.dropna(inplace = False)\n",
    "print(first_diff.head())\n",
    "\n",
    "roll_stats(first_diff,window = 12, cutoff = 0.01)\n",
    "dickey_fuller_test(first_diff, window = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the time series is stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ACF/PACF charts and find optimal parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(train_final.sales, lags=40, ax=ax1) \n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(train_final.sales, lags=40, ax=ax2)     # lags=40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After diffrencing-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(first_diff, lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(first_diff, lags=40, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the acf and pacf both has a recurring pattern every 7 periods. Indicating a weekly pattern exists.        \n",
    "Any time you see a regular pattern like that in one of these plots, you should suspect that there is some sort of significant seasonal thing going on. Then we should start to consider SARIMA to take seasonality into accuont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining order p, d, q through ACF/PACF plots:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to determin I. In our case, we see the first order differencing make the ts stationary. **I = 1**.\n",
    "\n",
    "AR model might be investigated first with lag length selected from the PACF or via empirical investigation. In our case, it's clearly that within 6 lags the AR is significant. Which means, we can use **AR = 6**\n",
    "\n",
    "To avoid the potential for incorrectly specifying the MA order (in the case where the MA is first tried then the MA order is being set to 0), it may often make sense to extend the lag observed from the last significant term in the PACF.\n",
    "\n",
    "What is interesting is that when the AR model is appropriately specified, the the residuals from this model can be used to directly observe the uncorrelated error. This residual can be used to further investigate alternative MA and ARMA model specifications directly by regression.\n",
    "\n",
    "Assuming an AR(s) model were computed, then I would suggest that the next step in identification is to estimate an MA model with s-1 lags in the uncorrelated errors derived from the regression. The parsimonious MA specification might be considered and this might be compared with a more parsimonious AR specification. Then ARMA models might also be analysed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of the AR term (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_model = sm.tsa.ARIMA(endog=train_final.sales, order=(6,1,0)).fit()\n",
    "print(arima_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the result              \n",
    "To see how our first model perform, we can plot the residual distribution. See if it's normal dist. And the ACF and PACF. For a good model, we want to see the residual is normal distribution. And ACF, PACF has not significant terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arima_model = sm.tsa.ARIMA(train_final.sales, (7,1,0)).fit(disp=False)\n",
    "# print(arima_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arima_model = sm.tsa.ARIMA(train_final.sales, (6,1,1)).fit(disp=False)\n",
    "# print(arima_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "resid = arima_model.resid\n",
    "print(normaltest(resid))\n",
    "# returns a 2-tuple of the chi-squared statistic, and the associated p-value. the p-value is very small, meaning\n",
    "# the residual is not a normal distribution\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax0 = fig.add_subplot(111)\n",
    "\n",
    "sns.distplot(resid ,fit = stats.norm, ax = ax0) # need to import scipy.stats\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = stats.norm.fit(resid)\n",
    "\n",
    "#Now plot the distribution using \n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual distribution')\n",
    "\n",
    "\n",
    "# ACF and PACF\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(arima_model.resid, lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(arima_model.resid, lags=40, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the graph looks very like a normal distribution. But it failed the test. Also we see a recurring correlation exists in both ACF and PACF. So we need to deal with seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using ARIMA Model\n",
    "Take the last 30 days in training set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 1726\n",
    "end_index = 1826\n",
    "train_df['forecast'] = arima_model.predict(start = start_index, end= end_index, dynamic= True)  \n",
    "train_df[start_index:end_index][['sales', 'forecast']].plot(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider seasonality affect by SARIMA¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will use SARIMAX\n",
    "\n",
    "sarima_model = sm.tsa.statespace.SARIMAX(train_final.sales, trend='n', order=(6,1,0)).fit()\n",
    "print(sarima_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = sarima_model.resid\n",
    "print(normaltest(resid))\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax0 = fig.add_subplot(111)\n",
    "\n",
    "sns.distplot(resid ,fit = stats.norm, ax = ax0) # need to import scipy.stats\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = stats.norm.fit(resid)\n",
    "\n",
    "#Now plot the distribution using \n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual distribution')\n",
    "\n",
    "# ACF and PACF\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(sarima_model.resid, lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(sarima_model.resid, lags=40, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using SARIMA Model\n",
    "Take the last 30 days in training set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 1730\n",
    "end_index = 1826\n",
    "train_df['forecast'] = sarima_model.predict(start = start_index, end= end_index, dynamic= True)  \n",
    "train_df[start_index:end_index][['sales', 'forecast']].plot(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this model is better than simple ARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_kun(y_true, y_pred):\n",
    "    mape = np.mean(abs((y_true-y_pred)/y_true))*100\n",
    "    smape = np.mean((np.abs(y_pred - y_true) * 200/ (np.abs(y_pred) + np.abs(y_true))).fillna(0))\n",
    "    print('MAPE: %.2f %% \\nSMAPE: %.2f'% (mape,smape), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_kun(train_df[1730:1825]['sales'],train_df[1730:1825]['forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
