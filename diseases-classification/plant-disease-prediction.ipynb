{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\"/mnt/share/datasets/diseases-classification/modelalexnet-and-best-weights-9/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Keras libraries and packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Initializing the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Convolution Step 1\n",
    "classifier.add(Convolution2D(96, 11, strides = (4, 4), padding = 'valid', input_shape=(224, 224, 3), activation = 'relu'))\n",
    "\n",
    "# Max Pooling Step 1\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "# Convolution Step 2\n",
    "classifier.add(Convolution2D(256, 11, strides = (1, 1), padding='valid', activation = 'relu'))\n",
    "\n",
    "# Max Pooling Step 2\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding='valid'))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "# Convolution Step 3\n",
    "classifier.add(Convolution2D(384, 3, strides = (1, 1), padding='valid', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "# Convolution Step 4\n",
    "classifier.add(Convolution2D(384, 3, strides = (1, 1), padding='valid', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "# Convolution Step 5\n",
    "classifier.add(Convolution2D(256, 3, strides=(1,1), padding='valid', activation = 'relu'))\n",
    "\n",
    "# Max Pooling Step 3\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), padding = 'valid'))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "# Flattening Step\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Full Connection Step\n",
    "classifier.add(Dense(units = 4096, activation = 'relu'))\n",
    "classifier.add(Dropout(0.4))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dense(units = 4096, activation = 'relu'))\n",
    "classifier.add(Dropout(0.4))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dense(units = 1000, activation = 'relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dense(units = 38, activation = 'softmax'))\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_weights('/mnt/share/datasets/diseases-classification/modelalexnet-and-best-weights-9/best_weights_9.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "from keras import layers\n",
    "for i, layer in enumerate(classifier.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we chose to train the top 2 conv blocks, i.e. we will freeze\n",
    "# the first 8 layers and unfreeze the rest:\n",
    "print(\"Freezed layers:\")\n",
    "for i, layer in enumerate(classifier.layers[:20]):\n",
    "    print(i, layer.name)\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainable parameters decrease after freezing some bottom layers   \n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the Model\n",
    "from keras import optimizers\n",
    "classifier.compile(optimizer=optimizers.SGD(learning_rate=0.001, momentum=0.9, decay=0.005),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 128\n",
    "base_dir = \"/mnt/share/datasets/diseases-classification/new-plant-diseases-dataset\"\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(base_dir+'/train',\n",
    "                                                 target_size=(224, 224),\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=False)\n",
    "\n",
    "valid_set = valid_datagen.flow_from_directory(base_dir+'/valid',\n",
    "                                            target_size=(224, 224),\n",
    "                                            batch_size=batch_size,\n",
    "                                            class_mode='categorical',\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = training_set.class_indices\n",
    "print(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = list(class_dict.keys())\n",
    "print(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = list(class_dict.keys())\n",
    "print(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = training_set.samples\n",
    "valid_num = valid_set.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "weightpath = \"best_weights_9.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weightpath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "#fitting images to CNN\n",
    "history = classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch=150, #train_num//batch_size,\n",
    "                         validation_data=valid_set,\n",
    "                         epochs=10, #25,\n",
    "                         validation_steps=100, #valid_num//batch_size,\n",
    "                         callbacks=callbacks_list)\n",
    "#saving model\n",
    "filepath=\"AlexNetModel.hdf5\"\n",
    "classifier.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting training values\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "#accuracy plot\n",
    "plt.plot(epochs, acc, color='green', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "#loss plot\n",
    "plt.plot(epochs, loss, color='pink', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, color='red', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting an image\n",
    "from keras.utrils import image\n",
    "import numpy as np\n",
    "image_path = \"/mnt/share/datasets/diseases-classification/new-plant-diseases-dataset/test/TomatoEarlyBlight1.JPG\"\n",
    "new_img = image.load_img(image_path, target_size=(224, 224))\n",
    "img = image.img_to_array(new_img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = img/255\n",
    "\n",
    "print(\"Following is our prediction:\")\n",
    "prediction = classifier.predict(img)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "d = prediction.flatten()\n",
    "j = d.max()\n",
    "for index,item in enumerate(d):\n",
    "    if item == j:\n",
    "        class_name = li[index]\n",
    "\n",
    "##Another way\n",
    "# img_class = classifier.predict_classes(img)\n",
    "# img_prob = classifier.predict_proba(img)\n",
    "# print(img_class ,img_prob )\n",
    "\n",
    "\n",
    "#ploting image with predicted class name        \n",
    "plt.figure(figsize = (4,4))\n",
    "plt.imshow(new_img)\n",
    "plt.axis('off')\n",
    "plt.title(class_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting an image\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "image_path = \"/mnt/share/datasets/diseases-classification/new-plant-diseases-dataset/test/AppleCedarRust1.JPG\"\n",
    "new_img = image.load_img(image_path, target_size=(224, 224))\n",
    "img = image.img_to_array(new_img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = img/255\n",
    "\n",
    "print(\"Following is our prediction:\")\n",
    "prediction = classifier.predict(img)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "d = prediction.flatten()\n",
    "j = d.max()\n",
    "for index,item in enumerate(d):\n",
    "    if item == j:\n",
    "        class_name = li[index]\n",
    "\n",
    "##Another way\n",
    "# img_class = classifier.predict_classes(img)\n",
    "# img_prob = classifier.predict_proba(img)\n",
    "# print(img_class ,img_prob )\n",
    "\n",
    "\n",
    "#ploting image with predicted class name        \n",
    "plt.figure(figsize = (4,4))\n",
    "plt.imshow(new_img)\n",
    "plt.axis('off')\n",
    "plt.title(class_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the test_data to start iterating over dataset from scratch\n",
    "valid_set.reset()\n",
    "# start to predict\n",
    "pred = classifier.predict(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "# use the confusion_matrix function provided by tensorflow to generate confusion matrix\n",
    "con_mat = tf.math.confusion_matrix(labels=valid_set.classes, predictions=np.argmax(pred, axis=1)).numpy()\n",
    "\n",
    "# normalize the confusion matrix\n",
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "# convert the nomalized confusion matrix for better view\n",
    "con_mat_df = pd.DataFrame(con_mat_norm,\n",
    "                     index = valid_set.class_indices.keys(), \n",
    "                     columns = valid_set.class_indices.keys())\n",
    "\n",
    "# show the nomalized confusion matrix\n",
    "con_mat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the original confusion matrix for better view (using the case numbers)\n",
    "con_mat_df_explain = pd.DataFrame(con_mat,\n",
    "                     index = valid_set.class_indices.keys(), \n",
    "                     columns = valid_set.class_indices.keys())\n",
    "\n",
    "# show the unnomalized confusion matrix\n",
    "con_mat_df_explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "# generate the clasification report by using the classification_report of sklearn package\n",
    "report = sklearn.metrics.classification_report(valid_set.classes, np.argmax(pred, axis=1), target_names=valid_set.class_indices.keys())\n",
    "\n",
    "# print the report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
